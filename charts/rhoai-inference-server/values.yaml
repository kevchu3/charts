serviceName: llama3
servingRuntime: dual-gpu-vllm-runtime
replicas: 1
GPUCount: 2
dataConnection:
  create: false
  name: inference-models
  modelPath: models/meta-llama/Meta-Llama-3.1-8B-Instruct/
resources:
  cpu: "2"
  mem: 35Gi
clusterSecretStoreName: eso-admin-secret-store