apiVersion: template.openshift.io/v1
kind: Template
metadata:
  annotations:
    opendatahub.io/apiProtocol: REST
    opendatahub.io/modelServingSupport: '["single"]'
  labels:
    opendatahub.io/dashboard: "true"
  name: qwen2-vllm-template
  namespace: redhat-ods-applications
objects:
- apiVersion: serving.kserve.io/v1alpha1
  kind: ServingRuntime
  metadata:
    annotations:
      opendatahub.io/recommended-accelerators: '["nvidia.com/gpu"]'
      openshift.io/display-name: Qwen2 vLLM ServingRuntime for KServe
    labels:
      opendatahub.io/dashboard: "true"
    name: qwen2-vllm-runtime
  spec:
    annotations:
      prometheus.io/path: /metrics
      prometheus.io/port: "8080"
    containers:
    - args:
      - --port=8080
      - --model=Qwen/Qwen2.5-7B-Instruct
      - --revision=bb46c15ee4bb56c5b63245ef50fd7637234d6f75
      - --served-model-name=Qwen2.5-7B-Instruct
      - --load-format=safetensors
      - --distributed-executor-backend=mp
      - --tensor_parallel_size=2
      command:
      - python
      - -m
      - vllm.entrypoints.openai.api_server
      - --trust-remote-code
      env:
      - name: HF_HOME
        value: /tmp/hf_home
      image: quay.io/modh/vllm@sha256:e14cae9114044dc9fe71e99c3db692a892b2caffe04283067129ab1093a7bde5
      name: kserve-container
      ports:
      - containerPort: 8080
        protocol: TCP
    multiModel: false
    supportedModelFormats:
    - autoSelect: true
      name: vLLM